{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMDb Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# https://imdbpy.readthedocs.io/en/latest/usage/data-interface.html\n",
    "from imdb import IMDb\n",
    "\n",
    "# plotting\n",
    "from IPython.core.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trakt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract these columns\n",
    "cols_trakt = ['title', 'tag', 'timestamp', 'duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace movie titles for correct querying OMDb\n",
    "correct_titles = {\n",
    "    # good matches but appear as different movies on list comparison (vs. - vs)\n",
    "    'Unbreakable Kimmy Schmidt: Kimmy vs. the Reverend':'Unbreakable Kimmy Schmidt: Kimmy vs the Reverend', \n",
    "    'The Mitchells vs. The Machines':'The Mitchells vs The Machines', \n",
    "    # mismatched titles\n",
    "    'Possessor Uncut':'Possessor', \n",
    "    \"Bram Stoker's Dracula\":'Dracula', \n",
    "    'Star Wars: The Rise of Skywalker':'Star Wars: Episode IX - The Rise of Skywalker',\n",
    "    'Pusher II: With Blood on My Hands':'Pusher II', \n",
    "    'Crip Camp: A Disability Revolution':'Crip Camp', \n",
    "    'Journey to the West: Conquering the Demons':'Journey to the West', \n",
    "    'One Child Nation':'Born in China',\n",
    "    'Charlie Countryman':'The Necessary Death of Charlie Countryman',  \n",
    "    'Biking Borders - eine etwas andere Reise':'Biking Borders',\n",
    "    'Born in China':'One Child Nation',\n",
    "    \n",
    "    # portuguese\n",
    "    'Soldier Millions':'Hero on the Front', \n",
    "\n",
    "    # spanish / basque\n",
    "    'Skins':'Pieles', \n",
    "    'Timecrimes':'Los cronocrímenes', \n",
    "    'Akelarre':'Coven',\n",
    "    \n",
    "    # french\n",
    "    'Atlantics':'Atlantique', \n",
    "    'Irreversible':'Irréversible',\n",
    "    'District B13':'Banlieue 13',\n",
    "  \n",
    "    # scandinavian\n",
    "    'Nightwatch':'Nattevagten',\n",
    "    \n",
    "    # other european languages\n",
    "    'Untamed Romania':'România neîmblânzitã', \n",
    "    'Solaris':'Solyaris',\n",
    "    'Apples':'Mila',\n",
    "    'Malena':'Malèna',\n",
    "    \n",
    "    # middle eastern\n",
    "    \"Where Is My Friend's House?\":\"Where Is the Friend's House?\", \n",
    "\n",
    "    # asian\n",
    "    'Sin-gwa ham-kke: Jwi-wa beol':'Along With the Gods: The Two Worlds', \n",
    "    'Sympathy for Lady Vengeance':'Lady Vengeance',\n",
    "}\n",
    "\n",
    "translations = {\n",
    "    'Ã†': 'Ae'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAKT class\n",
    "class Trakt:\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__df = pd.DataFrame()\n",
    "    \n",
    "    #\n",
    "    def load(self, filename):    \n",
    "        # Trakt connection with Zenobase (work with exported json file)        \n",
    "        # Load data using Python JSON module\n",
    "        with open(filename, 'r') as f:\n",
    "            json_data = json.loads(f.read())            \n",
    "            \n",
    "        # Normalizing data\n",
    "        df = self.__df.append(pd.json_normalize(json_data, record_path=['events']))\n",
    "        \n",
    "        # Select columns\n",
    "        cols = ['resource.title','tag','timestamp','duration']        \n",
    "        return df[cols].rename(columns={'resource.title': 'title'}).sort_values(by='timestamp').reset_index(drop=True)           \n",
    "\n",
    "    @staticmethod\n",
    "    def extract_year_from_title(x):\n",
    "        # Extract everything between two parenthesis\n",
    "        #df['year'] = df['title'].str.extract('\\((.*?)\\)')\n",
    "        return x.str.extract('\\((\\d+)\\)')\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_duration(x):\n",
    "        return pd.to_numeric(x / 1000 / 60).astype(int)    \n",
    "    \n",
    "    def convert_timestamp_to_datetime(self, s):\n",
    "        # timestamp series (s) as input \n",
    "        return pd.to_datetime(s, utc=True).apply(lambda x: x.replace(tzinfo=None))\n",
    "    \n",
    "    def convert_timestamp(self, x):    \n",
    "        # Convert timestamp to datetime - extract first, convert later\n",
    "        # https://programmersought.com/article/74164354616/\n",
    "        # .iloc[:,0] to get series from dataframe\n",
    "        return self.convert_timestamp_to_datetime(x.str.extract(r'(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2})').iloc[:,0])\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_title(x):\n",
    "        x = x.str.replace(' \\((.*?)\\)','')  \n",
    "        x = x.replace(translations, regex=True)\n",
    "        x = x.str.encode('latin-1').str.decode('utf8')\n",
    "        return x.replace(correct_titles, regex=True)   \n",
    "    \n",
    "    #\n",
    "    def convert_columns(self, df):\n",
    "        # Convert columns to appropriate formats\n",
    "        df['year'] = self.extract_year_from_title(df.title)\n",
    "        df['title'] = self.convert_title(df.title)\n",
    "        df['duration'] = self.convert_duration(df.duration)\n",
    "        df['timestamp'] = self.convert_timestamp(df.timestamp)\n",
    "        \n",
    "        return df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apiKey from datacamp\n",
    "# https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=7\n",
    "apiKey = '72bc447a'\n",
    "\n",
    "#Fetch Movie Data\n",
    "data_URL = 'http://www.omdbapi.com/?apikey='+apiKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titles that need query by iMDB id (generic name; hard to get correct info)\n",
    "id_titles = {\n",
    "    'Time':'tt11416746',\n",
    "    'Little Fish':'tt9735470',\n",
    "    'Ghost in the Shell':'tt0113568',\n",
    "    'Suspiria':'tt1034415',\n",
    "    'Patrick':'tt7618604',\n",
    "    'The Climb':'tt8637440',\n",
    "    'The Call':'tt10530176',\n",
    "    'The Silence':'tt1361835',\n",
    "    'The Man Who Saved the World':'tt2277106',\n",
    "    '1984':'tt0087803',\n",
    "    'Cruella':'tt3228774',\n",
    "    'Saint George':'tt4895668',\n",
    "    'Joy':'tt8917752',\n",
    "    'Luca':'tt12801262',\n",
    "    'Black Widow':'tt3480822',\n",
    "    'Fear Street Part One: 1994':'tt6566576',\n",
    "    'Fear Street Part Two: 1978':'tt9701940',\n",
    "    'Fear Street Part Three: 1666':'tt9701942',\n",
    "    'Twelve Monkeys':'tt0114746',\n",
    "    'The Life Ahead':'tt10627584',\n",
    "    'Vivo':'tt6338498',\n",
    "    'The Suicide Squad':'tt6334354',\n",
    "    'Shadow':'tt6864046',\n",
    "    'Kate':'tt7737528',\n",
    "    'Torment':'tt0109731',\n",
    "    \"Dogs Don't Wear Pants\":'tt9074574',\n",
    "    'Lux Aeterna':'tt10272534',\n",
    "    'Limbo':'tt9138170',\n",
    "    'Christine':'tt4666726',\n",
    "    'Finch':'tt3420504',\n",
    "    'Léon: The Professional':'tt0110413',\n",
    "    'Leviathan':'tt2802154',\n",
    "    'The Hunt':'tt8244784',\n",
    "    'On My Skin':'tt7121252',\n",
    "    'Los cronocrímenes':'tt0480669',\n",
    "    'Dune':'tt0087182',\n",
    "    'The Wind Rises':'tt2013293',\n",
    "    'Nineteen Eighty-Four':'tt0087803',\n",
    "    'Banlieue 13':'tt0414852',\n",
    "    \"Don't Look Up\":'tt11286314',\n",
    "    'Long Shot':'tt7344360',\n",
    "    'Rams':'tt3296658',\n",
    "    'The Wave':'tt3616916',\n",
    "    'The Guilty':'tt6742252', \n",
    "    'Shorta':'tt11081050',\n",
    "    'The Trip':'tt13109952',\n",
    "    'La Belle Verte':'tt0115650',\n",
    "    'Durante La Tormenta':'tt6908274',\n",
    "    'Atlantique':'tt10199586',\n",
    "    'The Necessary Death of Charlie Countryman':'tt1196948',\n",
    "    'Tigers Are Not Afraid':'tt4823434',\n",
    "    'Variações: Guardian Angel':'tt2155399',\n",
    "    'The Bar':'tt5121816',\n",
    "    'Crazy About Her':'tt11698630',\n",
    "    'New Order':'tt12474056',\n",
    "    \"Je T'Aime, Je T'Aime\":'tt0063152',\n",
    "    'Mirror':'tt0072443',\n",
    "    'Irréversible':'tt0290673',\n",
    "    'Solyaris':'tt0069293',\n",
    "    'Dr. Strangelove':'tt0057012',\n",
    "    'Born in China':'tt8923482',\n",
    "    'Boiling Point':'tt11127680',\n",
    "    'The Innocents':'tt4028464',\n",
    "    'Dracula':'tt0103874',\n",
    "    'Nightmare Alley':'tt7740496',\n",
    "    'Wolf':'tt10698174',\n",
    "    'Kimi':'tt14128670'\n",
    "}\n",
    "\n",
    "id_titles_query = [item for item in list(id_titles.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change back movie names for analysis\n",
    "# original titles : translated titles\n",
    "replace_titles = {\n",
    "    'Dracula':\"Bram Stoker's Dracula\",           \n",
    "    'România neîmblânzitã':'Untamed Romania',    \n",
    "    'Koirat eivät käytä housuja':\"Dogs Don't Wear Pants\",    \n",
    "    'Mila':'Apples',   \n",
    "    'Hauru no ugoku shiro':\"Howl's Moving Castle\",\n",
    "    'Gake no ue no Ponyo':'Ponyo',    \n",
    "    'Kôkaku Kidôtai':'Ghost in the Shell',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OMDb class\n",
    "class OMDB:\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__df = pd.DataFrame()\n",
    "    \n",
    "    #    \n",
    "    def convert_to_int(self, x):\n",
    "        # clean columns\n",
    "        # ['Year','Runtime','Metascore','imdbVotes']        \n",
    "        return x.replace('N/A', 0, regex=True).astype(int)\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_imdb_rating(x):\n",
    "        return x.apply(pd.to_numeric, errors='coerce').astype(float)\n",
    "    \n",
    "    def convert_imdb_votes(self, x):\n",
    "        # remove ',' from votes (ex: 6,000 to 6000)\n",
    "        return self.convert_to_int(x.astype(str).str.replace(',',''))\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_imdb_id(x):\n",
    "        # only numeric ID to join\n",
    "        return x.astype(str).str.replace('tt','')\n",
    "    \n",
    "    def convert_runtime(self, x):\n",
    "        # remove 'min', get only integers\n",
    "        return self.convert_to_int(x.astype(str).replace('N/A', '000 min', regex=True).str.replace(r'\\D', ''))\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_timestamp(x):\n",
    "        # remove timezone\n",
    "        return pd.to_datetime(x, utc=True).apply(lambda x: x.replace(tzinfo=None))\n",
    "\n",
    "    def convert_boxoffice(self, x):\n",
    "        # clean box office values\n",
    "        return self.convert_to_int(x.astype(str).str.lstrip('$').str.replace(',',''))\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_country(x):\n",
    "        # convert country names\n",
    "        replace_country = {\n",
    "            'United Kingdom':'UK',\n",
    "            'United States':'USA'\n",
    "        }\n",
    "        return x.replace(replace_country, regex=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_language(x):\n",
    "        # convert language names\n",
    "        replace_lang = {\n",
    "            'American Sign Language':'American Sign',\n",
    "            'American Sign':'American Sign Language',\n",
    "            'Korean Sign':'Korean Sign Language'\n",
    "        }\n",
    "        return x.replace(replace_lang, regex=True)\n",
    "    \n",
    "    #\n",
    "    def convert_columns(self, df):\n",
    "        # Convert columns to appropriate formats\n",
    "        df['Runtime'] = self.convert_runtime(df.Runtime)\n",
    "        df['Language'] = self.convert_language(df.Language)\n",
    "        df['Country'] = self.convert_country(df.Country)\n",
    "        df['Metascore'] = self.convert_to_int(df.Metascore) # convert_to_int\n",
    "        df['imdbRating'] = self.convert_imdb_rating(df.imdbRating)\n",
    "        df['imdbVotes'] = self.convert_imdb_votes(df.imdbVotes)\n",
    "        df['imdbID'] = self.convert_imdb_id(df.imdbID)\n",
    "        df['Timestamp'] = self.convert_timestamp(df.Timestamp)        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def retranslate_titles(x):   \n",
    "        # change back movie names for analysis\n",
    "        return x.replace(replace_titles)      \n",
    "    \n",
    "    # Connect with OMDb API\n",
    "    def request_db(self, df):        \n",
    "        response = []\n",
    "        movies = []\n",
    "        i=0\n",
    "        \n",
    "        df.reset_index(inplace=True) # to add timestamp later\n",
    "        nMovies = len(df)    \n",
    "        # get titles that need id query\n",
    "        match = list(set(id_titles_query).intersection(df.title))\n",
    "        \n",
    "        for i in range(nMovies):\n",
    "            params = {}\n",
    "            movieTitle = df['title'][i]\n",
    "            #yearTitle = df['year'][i] # for cases like 'Dune'\n",
    "            \n",
    "            if movieTitle in match:\n",
    "                params = {\n",
    "                    'type':'movie',\n",
    "                    'i':id_titles.get(movieTitle)    \n",
    "                }\n",
    "            else:\n",
    "                params = {\n",
    "                    't':movieTitle,\n",
    "                    'type':'movie',\n",
    "                    #'y':yearTitle\n",
    "                }\n",
    "            i=i+1;\n",
    "    \n",
    "            response = requests.get(data_URL, params=params).json()\n",
    "            movies.append(response)\n",
    "        \n",
    "        return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "testing = False\n",
    "\n",
    "if testing:\n",
    "    # single movie request\n",
    "    stitle = \"Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb\" #\"Kingdom: Ashin of the North\"\n",
    "    idd = 'tt3420504'\n",
    "    \n",
    "    params = {\n",
    "        't':stitle,\n",
    "        'type':'movie',\n",
    "        #'y': 2021 #syear\n",
    "        #'i':idd\n",
    "    }\n",
    "    \n",
    "    responseTest = requests.get(data_URL, params=params).json()\n",
    "    responseTest\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract these columns\n",
    "cols_imdb = ['title', 'year', 'imdbID', 'runtimes', 'genres', 'director', 'writer', 'cinematographers', 'cast', 'countries', 'languages', 'rating', 'votes', 'plot outline', 'production companies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDb class\n",
    "class IMDB:\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__df = pd.DataFrame(columns=cols_imdb)\n",
    "        \n",
    "        self.__cols_int = ['year','runtimes','votes']\n",
    "        self.__cols_str = ['imdbID','genres','director','writer','cinematographers','cast','countries','languages','production companies']\n",
    "    \n",
    "    #        \n",
    "    def load(self, filename):\n",
    "        # Load imdb file\n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    #@staticmethod\n",
    "    #def convert_year(x):\n",
    "    #    return x.fillna(0).astype(int)\n",
    "    #\n",
    "    #@staticmethod\n",
    "    #def convert_runtime(x):\n",
    "    #    return x.fillna(0).astype(int)\n",
    "    #\n",
    "    #@staticmethod\n",
    "    #def convert_imdb_votes(x):\n",
    "    #    return x.fillna(0).astype(int)\n",
    "    #\n",
    "    #@staticmethod\n",
    "    #def convert_imdb_id(x):\n",
    "    #    return x.astype(str)\n",
    "    #\n",
    "    #@staticmethod\n",
    "    #def convert_writer(x):\n",
    "    #    return x.astype(str)\n",
    "    \n",
    "    #\n",
    "    def convert_columns(self, df):\n",
    "        # Convert columns to appropriate formats\n",
    "        df[self.__cols_int] = df[self.__cols_int].fillna(0).astype(int)\n",
    "        df[self.__cols_str] = df[self.__cols_str].fillna('N/A').astype(str)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    @staticmethod    \n",
    "    def get_info_list(x, cols, d):\n",
    "        # Extract information from imdb ia object\n",
    "        # x - series\n",
    "        # cols - columns\n",
    "        # d - dictionary\n",
    "        for i in range(len(cols)):\n",
    "            if x[cols[i]] != None:\n",
    "                d[cols[i]] = x[cols[i]]\n",
    "            else:\n",
    "                if (cols[i] == 'rating') or (cols[i] == 'votes'):\n",
    "                    d[cols[i]] = 0\n",
    "                elif cols[i] == 'plot outline':\n",
    "                    d[cols[i]] = 'N/A'\n",
    "                else:\n",
    "                    d[cols[i]] = ['N/A']            \n",
    "        return d\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_name_list(x, cols, d):\n",
    "        # Extract information from imdb ia object\n",
    "        # x - series\n",
    "        # cols - columns\n",
    "        # d - dictionary\n",
    "        for i in range(len(cols)):\n",
    "            if x[cols[i]] != None:\n",
    "                temp = []\n",
    "                for info in x[cols[i]]:\n",
    "                    if info != None:\n",
    "                        temp.append(info)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                # sometimes same person appears multiple times\n",
    "                if cols[i] == 'writer':\n",
    "                    # clean list\n",
    "                    d[cols[i]] = list(filter(None, list(set(temp))))\n",
    "                else:\n",
    "                    d[cols[i]] = temp \n",
    "          \n",
    "            else:\n",
    "                d[cols[i]] = ['N/A']            \n",
    "        return d\n",
    "\n",
    "    # Clean output from db request    \n",
    "    def clean_df(self, movies):  \n",
    "        df = pd.DataFrame(columns=cols_imdb)\n",
    "        \n",
    "        # columns\n",
    "        cols_info = ['runtimes','genres','countries','languages','rating','votes','plot outline']\n",
    "        cols_name = ['director','writer','cinematographers','cast','production companies']\n",
    "    \n",
    "        # dictionaries\n",
    "        dict_info = {}\n",
    "        dict_name = {}\n",
    "        \n",
    "        for i in range(len(movies)):\n",
    "            # intiliaze\n",
    "            temp_movie = []\n",
    "            movie = movies[i]\n",
    "\n",
    "            # set values\n",
    "            title = movie['title']\n",
    "            year = movie['year']\n",
    "            imdbID = movie['imdbID']\n",
    "            \n",
    "            dict_info = self.get_info_list(movie, cols_info, dict_info)\n",
    "            dict_name = self.get_name_list(movie, cols_name, dict_name)\n",
    "    \n",
    "            # organize columns according to DF\n",
    "            temp_movie = [title, year, imdbID, \n",
    "                          dict_info['runtimes'], dict_info['genres'], \n",
    "                          dict_name['director'], dict_name['writer'],\n",
    "                          dict_name['cinematographers'], dict_name['cast'],\n",
    "                          dict_info['countries'], dict_info['languages'], \n",
    "                          dict_info['rating'], dict_info['votes'], \n",
    "                          dict_info['plot outline'], dict_name['production companies']\n",
    "                         ]\n",
    "        \n",
    "            df.loc[len(df)] = temp_movie\n",
    "       \n",
    "        df = list_to_string(df, cols_info[:-3])\n",
    "        df = list_to_string(df, cols_name)    \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    #\n",
    "    def add_movies(self, df_trakt, df):\n",
    "        # add new movies (trakt - imdb) to list\n",
    "        titles = df_trakt['title']\n",
    "        add_titles = []\n",
    "        \n",
    "        # index differences (number of movies added)\n",
    "        idxdif = titles.last_valid_index() - df['og'].last_valid_index()\n",
    "        \n",
    "        # if new movies\n",
    "        if idxdif != 0:\n",
    "            # get titles\n",
    "            add_titles = titles.iloc[-idxdif:].values + ' (' + df_trakt.iloc[-idxdif:]['year'].values + ')'\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        return add_titles\n",
    "    \n",
    "    # Connect with IMDb API\n",
    "    def request_df(self, movies, ids=False):\n",
    "        all_movies = []    \n",
    "        \n",
    "        # create an instance of the IMDb class\n",
    "        ia = IMDb()\n",
    "        \n",
    "        for i in range(len(movies)):\n",
    "            search = []\n",
    "                  \n",
    "            # if searching by title (not searching by imdbID)\n",
    "            if ids == False:\n",
    "                # better to search by title + year : get first result\n",
    "                search = ia.search_movie(movies[i]) # Dune case (multiple movies)        \n",
    "                temp_title = movies[i].split(' (')[0]\n",
    "                \n",
    "                if (search == []) or (not search[0]['title'].startswith(temp_title)): # if search result is empty or title doesn't start with the name of the movie\n",
    "                    search = ia.search_movie(temp_title)        \n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                # Select correct title\n",
    "                temp = search[0]    \n",
    "                ## Fetch data from correct title and update info into var\n",
    "                ia.update(temp, info='main', override=1)\n",
    "                \n",
    "            # if searching by imdbID verification is not needed\n",
    "            else:\n",
    "                temp = ia.get_movie(movies[i], info='main')\n",
    "            \n",
    "            # gather data to export\n",
    "            all_movies.append({key: temp.get(key) for key in cols_imdb})\n",
    "            \n",
    "        return all_movies    \n",
    "    \n",
    "    # Correct IMDb data based on trakt's current history\n",
    "    def correct_movie_data(df_trakt, df_imdb):\n",
    "        # Copy df to update results\n",
    "        df_updated = df_imdb.copy()\n",
    "        # Create dfs\n",
    "        df_wrong_id = pd.DataFrame()\n",
    "        df_wrong_title = pd.DataFrame()\n",
    "        #\n",
    "        replace_by_id = []\n",
    "        replace_by_title = []\n",
    "        # Title column to get which movies to correct\n",
    "        col_title = ['resource.title']\n",
    "        \n",
    "        ## Prepare dfs to merge\n",
    "        df1 = df_trakt[['resource.title','tag']].reset_index()\n",
    "        df2 = df_imdb[['title','year','runtimes','genres','director','countries','rating']].reset_index()\n",
    "        # merge dfs\n",
    "        df_merge = df1.merge(df2, on='index')\n",
    "        df_merge.drop('index', axis=1, inplace=True)\n",
    "        \n",
    "        ## Get list of movies to correct\n",
    "        # common results of wrong requests\n",
    "        list_wrong_genre = ['Talk-Show','Short']\n",
    "        wrong_genre = df_merge[df_merge.genres.str.contains('|'.join(list_wrong_genre), na=False)]    \n",
    "        # drop shorts from these directors - correct results already\n",
    "        drop_director_shorts = ['David Lynch','Paul Thomas Anderson','Denis Villeneuve','Yorgos Lanthimos','Vi-Dan Tran']\n",
    "        wrong_genre = wrong_genre[~wrong_genre.director.str.contains('|'.join(drop_director_shorts), na=False)][col_title]\n",
    "        \n",
    "        # runtimes = NaN\n",
    "        wrong_runtime = df_merge[df_merge['runtimes'].isnull()][col_title]\n",
    "        # director = NaN\n",
    "        wrong_director = df_merge[df_merge.director.isna()][col_title]\n",
    "        \n",
    "        ## Combine results of titles with wrong information\n",
    "        # https://stackoverflow.com/questions/34989341/how-to-remove-nan-value-while-combining-two-column-in-panda-data-frame\n",
    "        # First concat\n",
    "        wrong_movies = pd.concat([wrong_genre, wrong_runtime], axis=1)\n",
    "        wrong_movies = wrong_movies.bfill(axis=1).iloc[:, 0]\n",
    "        # Second concat\n",
    "        wrong_movies = pd.concat([wrong_movies, wrong_director], axis=1)\n",
    "        wrong_movies = wrong_movies.bfill(axis=1).iloc[:, 0]\n",
    "        # Convert to df\n",
    "        wrong_movies = pd.DataFrame(wrong_movies)\n",
    "        \n",
    "        ## Prepare titles to search by title without (year)\n",
    "        # strip (year)\n",
    "        wrong_movies['og'] = wrong_movies['resource.title'].str.split('(').str[0].str.strip()\n",
    "        wrong_movies.drop('resource.title', axis=1, inplace=True)\n",
    "        \n",
    "        ## FIRST UPDATE\n",
    "        ## Request by ID\n",
    "        ## remove 'tt' from imdbid to request api\n",
    "        # '1361835':'The Silence',\n",
    "        imdbID = {'13947978':'Accomplice', \n",
    "                  '6738136':'Revenge'}\n",
    "        \n",
    "        # List of ids\n",
    "        movie_ids = list(imdbID.keys())    \n",
    "        # request by ids\n",
    "        df_wrong_id = clean_iMDB(request_iMDB(movie_ids, True))          \n",
    "        # Update df\n",
    "        for i in range(len(df_wrong_id)):\n",
    "            replace_by_id.append(wrong_movies[wrong_movies.og.str.contains(df_wrong_id.iloc[i].title)].index[0])\n",
    "            ## correct on df\n",
    "            df_updated.loc[replace_by_id[i], :] = df_wrong_id.loc[i]\n",
    "    \n",
    "        ## SECOND UPDATE    \n",
    "        # Now remove movies requested by ID from list\n",
    "        wrong_movies.drop(index=replace_by_id, inplace=True)   \n",
    "        # Request by title without (year)\n",
    "        df_wrong_title = clean_iMDB(request_iMDB(wrong_movies.og.values))\n",
    "        # Update df     \n",
    "        for j in range(len(df_wrong_title)):\n",
    "            replace_by_title.append(wrong_movies[wrong_movies.og.str.contains(df_wrong_title.iloc[j].title)].index[0])\n",
    "            ## correct on df\n",
    "            df_updated.loc[replace_by_title[j], :] = df_wrong_title.loc[j]\n",
    "    \n",
    "        return df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column of lists into column of strings\n",
    "def list_to_string(df, cols):\n",
    "    for i in range(len(cols)):\n",
    "        df[cols[i]] = df[cols[i]].agg(lambda x: ', '.join(map(str, x)))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total movies watched by Year\n",
    "def totals_by_year(df, all_watched=False, docs=False):\n",
    "    df = df[['Title','Genre','Director','Date']].copy()\n",
    "    # Get documentaries\n",
    "    df_genres = df.Genre.str.contains('Documentary')\n",
    "    \n",
    "    if all_watched:\n",
    "        df_ = df.copy()\n",
    "    else:\n",
    "        if docs:\n",
    "            df_ = df[df_genres] # include documentaries\n",
    "        else:\n",
    "            df_ = df[~df_genres] # do not include documentaries\n",
    "\n",
    "    # Get year\n",
    "    df_['Year'] = df_.Date.dt.year\n",
    "    df_.drop(columns='Date', inplace=True)\n",
    "    # To avoid incorrect match from movies with same title (watched in the same year, like Swan Song (2021))\n",
    "    # Add 'Director' to 'Title' to create unique string\n",
    "    df_['Title'] = df_['Title'] + ' - ' + df_['Director']\n",
    "    df_.drop(columns='Director', inplace=True)\n",
    "    \n",
    "    # Group by Year\n",
    "    df_by_year = df_.groupby(by='Year').count()\n",
    "    # Unique and count all movies watched\n",
    "    df_by_year_uc = df_.groupby(by='Year').agg({'Title':['nunique','count']})\n",
    "    \n",
    "    # Add total row\n",
    "    rowtotal = df_by_year_uc.sum()\n",
    "    rowtotal.name = 'All'\n",
    "    df_by_year_uc.append(rowtotal)\n",
    "    \n",
    "    return df_by_year, df_by_year_uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if size is different (new movies viewed)\n",
    "# last exported file size (new json file from zenobase)\n",
    "def check_new_movies(foo, df):\n",
    "    newMovies = 0\n",
    "    cols = ['title','year']\n",
    "    \n",
    "    if type(foo) == pd.core.frame.DataFrame and (len(foo) != len(df)):\n",
    "        newMovies = len(df.title) - len(foo.Title) \n",
    "        \n",
    "        # get added movies\n",
    "        if newMovies != 0:\n",
    "            diffMovies = list(set(df.title.str.lower()) - set(foo.Title.str.lower()))\n",
    "            \n",
    "            diffMovies = [item.lower() for item in diffMovies]\n",
    "            df_diffMovies = df[df['title'].str.lower().str.contains('|'.join(diffMovies))]\n",
    "    \n",
    "            ## Select data to search in OMDB\n",
    "            data = df_diffMovies[cols]\n",
    "                        \n",
    "    elif type(foo) != pd.core.frame.DataFrame:\n",
    "        print('Get everything!')\n",
    "        \n",
    "        ## Select data to search in OMDB\n",
    "        data = df[cols]\n",
    "        \n",
    "    else:\n",
    "        print('Do nothing.')\n",
    "        data = 0\n",
    "        pass\n",
    "    \n",
    "    return data, newMovies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check new data with existing df from pickle file\n",
    "def look_iama_pickle(foo, df_trakt, movies, newMovies, index, forceRequest):\n",
    "    \n",
    "    # copy foo if pickle exists and there's no new movies\n",
    "    if type(foo) == pd.core.frame.DataFrame and newMovies == 0 and forceRequest == False:\n",
    "        df = foo.copy()\n",
    "        df['Timestamp'] = pd.Series(df_trakt['timestamp'])\n",
    "        \n",
    "    elif type(foo) == pd.core.frame.DataFrame and newMovies != 0 and forceRequest == False:\n",
    "        index = index[-newMovies:]\n",
    "        df_movies = pd.DataFrame(movies)\n",
    "        df_movies = df_movies.tail(newMovies).reset_index(drop=True)\n",
    "        df_movies['Timestamp'] = pd.Series(df_trakt.iloc[index].reset_index()['timestamp']) \n",
    "        \n",
    "        df_movies['Date'] = df_movies.Timestamp.dt.date\n",
    "        df_movies['Date'] = pd.to_datetime(df_movies['Date'])\n",
    "        # extracting time from timestamp\n",
    "        df_movies['Time'] = [dt.datetime.time(d) for d in df_movies['Timestamp']]\n",
    "        \n",
    "        df = foo.append(df_movies)\n",
    "    \n",
    "    elif type(foo) != pd.core.frame.DataFrame or forceRequest == True:\n",
    "        df = pd.DataFrame(movies)\n",
    "        # use OMDB query result and convert to dataframe\n",
    "        df['Timestamp'] = pd.Series(df_trakt['timestamp'])\n",
    "        df['Timestamp'] = pd.to_datetime(df.Timestamp, utc=True)\n",
    "        \n",
    "        df['Date'] = df.Timestamp.dt.date\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        # extracting time from timestamp\n",
    "        df['Time'] = [dt.datetime.time(d) for d in df['Timestamp']]\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single movies hard correct\n",
    "def hard_correct_movie_data(df, loc, idt, api):\n",
    "    temp = []\n",
    "    response = []\n",
    "    \n",
    "    if api == 'omdb':\n",
    "        response = requests.get(data_URL, params={'type':'movie', 'i':idt}).json()\n",
    "        temp.append(response)\n",
    "        \n",
    "        correct = pd.DataFrame(temp)[all_cols[:-3]]\n",
    "        df.loc[loc, all_cols[:-3]] = correct.loc[0] # only\n",
    "    \n",
    "    elif api == 'imdb':\n",
    "        idt = idt.replace(r'tt', '')\n",
    "        ia = IMDb()\n",
    "        response = ia.get_movie(idt, info='main')\n",
    "        temp.append({key: response.get(key) for key in cols_imdb})\n",
    "\n",
    "        correct = imdb.clean_df(temp)\n",
    "        df.iloc[loc, 1:] = correct.loc[0]\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of preparation for \"exploding\"\n",
    "# Convert string with multiple values to array of values\n",
    "# df_explode_genre.Genre = df_explode_genre.Genre.str.split(',').apply(lambda x: [e.strip() for e in x])\n",
    "\n",
    "# https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\n",
    "def explode(df, lst_cols, fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    if (lst_cols is not None\n",
    "        and len(lst_cols) > 0\n",
    "        and not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode date into multiple columns\n",
    "def explode_date(df):\n",
    "    df['DateYear'] = df.Date.dt.year\n",
    "    \n",
    "    df['Month'] = df.Date.dt.month\n",
    "    df['MonthName'] = df.Date.dt.month_name()\n",
    "    \n",
    "    df['Weekday'] = df.Date.dt.weekday\n",
    "    df['WeekdayName'] = df.Date.dt.strftime(\"%A\")\n",
    "    \n",
    "    df['Day'] = df.Date.dt.day\n",
    "    #df.drop(columns='Date', inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def skip_days(df, calendar, year):\n",
    "    # explode df\n",
    "    df_explode = explode_date(df)\n",
    "    # filter by year\n",
    "    df_explode = df_explode[df_explode.Date.dt.year == year]\n",
    "    # crosstab days by month\n",
    "    df_explode = pd.crosstab(df_explode.Day, [df_explode.Month, df_explode.MonthName])\n",
    "    \n",
    "    # get months\n",
    "    col_index = df_explode.columns\n",
    "       \n",
    "    # get days in month of complete months in year\n",
    "    calyear = calendar[calendar.date.dt.year == year].reset_index(drop=True)\n",
    "    days_in_months = calyear.daysinmonths\n",
    "    \n",
    "    # months complete this year\n",
    "    months = len(calyear)\n",
    "    \n",
    "    # initialize\n",
    "    no_moviedays = []\n",
    "    pct_no_moviedays = 0\n",
    "    \n",
    "    for i in range(months):\n",
    "        moviedays = 0\n",
    "        # sum all days per month with 0 - False (no movies watched)\n",
    "        moviedays = df_explode[col_index[i]].astype(bool).sum(axis=0)\n",
    "        # append sum to array (sum per month)\n",
    "        no_moviedays.append(int(days_in_months[i] - moviedays))\n",
    "    \n",
    "    # Percentage of days per year without a movie watched\n",
    "    if days_in_months.sum() != 0:\n",
    "        pct_no_moviedays = int((sum(no_moviedays) / days_in_months.sum()) * 100)\n",
    "    else:\n",
    "        pct_no_moviedays = 0\n",
    "        \n",
    "    print(str(year))\n",
    "    print('By month: ' + str(no_moviedays))\n",
    "    print('Total: ' + str(sum(no_moviedays)) + '/' + str(calyear.daysinmonths.sum()))\n",
    "    print(str(pct_no_moviedays) + ' %')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crosstab filtering by year\n",
    "def crosstab_by_year(df, index=None, year=None):\n",
    "    # if year=None get all years\n",
    "    cols = ['Title','Date']\n",
    "    \n",
    "    if year != None:\n",
    "        df_ = df[df.Date.dt.year == year][cols].copy()\n",
    "    else:\n",
    "        df_ = df[cols].copy()\n",
    "    \n",
    "    df_ = explode_date(df_)\n",
    "    if index == None:\n",
    "        # Movies by month/year\n",
    "        multiIndex = df_['DateYear']\n",
    "    else:\n",
    "        multiIndex = [df_[index], df_[index+'Name']]\n",
    "    \n",
    "    if index == 'Weekday' or index == None:\n",
    "        ct = pd.crosstab(multiIndex, [df_['Month'], df_['MonthName']], margins=True)\n",
    "    elif index == 'Month':\n",
    "        ct = pd.crosstab(multiIndex, df_['Day'])\n",
    "    \n",
    "    return ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie ratio by movies watched on weekends vs movies watched on weekdays\n",
    "def ratio_weekend(df, year=None):    \n",
    "    df_ = create_weekday_df(df, year)\n",
    "\n",
    "    # Create Weekdays df with weekdays' names\n",
    "    allweek = df_.Weekday.values #['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']    \n",
    "    df_week = pd.DataFrame(0, index=allweek, columns=['Movies'])    \n",
    "    df_week.reset_index(inplace=True)\n",
    "    df_week.rename(columns={\"index\": \"Weekday\"}, inplace=True)\n",
    "\n",
    "    # Concatenate dfs\n",
    "    df_concat = pd.concat([df_week, df_]).groupby(['Weekday']).sum().reset_index()\n",
    "    df_concat = df_concat.set_index('Weekday').loc[allweek]\n",
    "\n",
    "    # Weekdays vs Weekend\n",
    "    dropdays = allweek[1:-1] # ['Tuesday','Wednesday','Thursday','Friday','Saturday']\n",
    "    # weekdays sum\n",
    "    df_concat.loc['Monday'] += df_concat.iloc[1:5].sum()\n",
    "    # weekend sum\n",
    "    df_concat.loc['Sunday'] += df_concat.iloc[5].sum()\n",
    "    # drop unecessary columns\n",
    "    df_concat.drop(dropdays, inplace=True)\n",
    "    # rename columns\n",
    "    df_concat.rename(index={'Monday': 'Weekdays'}, inplace=True)\n",
    "    df_concat.rename(index={'Sunday': 'Weekend'}, inplace=True)\n",
    "    \n",
    "    # Get ratio\n",
    "    ratio = df_concat.reset_index()['Movies']\n",
    "    pct_ratio = ((ratio.iloc[1] / (ratio.iloc[0] + ratio.iloc[1])) * 100).astype(int)    \n",
    "    print(str(pct_ratio) + '% of the movies were watched on the weekend!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of string\n",
    "def counter_display(df, by, col, top=None):\n",
    "    ## Fill with 'nan'\n",
    "    #df[by] = df[by].fillna('nan')\n",
    "\n",
    "    # Check type\n",
    "    sample = df[by].sample().values[0]\n",
    "    \n",
    "    if isinstance(sample, str):\n",
    "        series = pd.Series(df[by].str.split(', ').apply(lambda x: [e.strip() for e in x]).map(Counter).sum())\n",
    "    else: # list\n",
    "        series = pd.Series(df[by].value_counts())\n",
    "        \n",
    "    # Organize df to export\n",
    "    df_export = pd.DataFrame(series, columns=[col])\n",
    "    \n",
    "    if top == None:\n",
    "        # Everything\n",
    "        df_export = df_export.sort_values(by=col, ascending=False)\n",
    "    else:\n",
    "        # Show top\n",
    "        df_export = df_export.nlargest(top, col, keep='all')\n",
    "   \n",
    "    return df_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average movie rating by column\n",
    "# If column values have multiple substrings in a string (examples: 'Genre', 'Actors', etc) use explode() to separate all values\n",
    "def get_mean_value(df, by, get_avg, top=None):   \n",
    "#    ## Set max limit to 60 persons (cast members)\n",
    "#    LIMIT = 60\n",
    "\n",
    "    # Create df to explode by 'by'\n",
    "    df_explode = df.copy() # remove some not needed columns\n",
    "    # Split multiple persons in one string\n",
    "    df_explode[by] = df_explode[by].str.split(', ').apply(lambda x: [e.strip() for e in x])\n",
    "#    ## Set limit\n",
    "#    df_explode[by] = df_explode[by].transform(lambda x: x[:LIMIT])\n",
    "    # Remove duplicates\n",
    "    df_explode[by] = df_explode[by].map(lambda x: list(set(x)))\n",
    "    # Explode df base on 'by'\n",
    "    df_explode = explode(df_explode, [by])\n",
    "    \n",
    "    df_export = df_explode.groupby(by).agg({'Title':[list,'count'], get_avg:'mean'})\n",
    "    df_export = df_export.droplevel(0, axis=1)\n",
    "    df_export = df_export.rename(columns={'list':'Titles', 'count':'Total', 'mean':get_avg})\n",
    "    \n",
    "    # if 'imdbRating'\n",
    "    if get_avg.find('Rating') != -1:\n",
    "        df_export[get_avg] = round(df_export[get_avg], 2)\n",
    "    # else 'Runtime' or 'imdbVotes'\n",
    "    else:\n",
    "        df_export[get_avg] = df_export[get_avg].astype(int)\n",
    "         \n",
    "    if top == None:\n",
    "        # Everything\n",
    "        df_export = df_export.sort_values('Total', ascending=False)\n",
    "    else:\n",
    "        # Show top\n",
    "        df_export = df_export.nlargest(top, 'Total', keep='all')\n",
    "    \n",
    "    return df_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies by actor\n",
    "def count_actor(series, top):     \n",
    "    seriesNew = []\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        # initialize an empty string\n",
    "        series.iloc[i]\n",
    "        str1 = ', ' .join(series.iloc[i])\n",
    "        seriesNew.append(Counter([x.strip() for x in str1.split(',')]).most_common(top))\n",
    "                      \n",
    "    return pd.Series(seriesNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair director - actor\n",
    "def director_actors(data, col, top):\n",
    "    colList = counter_display(data, col, 'Actors', None).reset_index()['index']\n",
    "    d = {}\n",
    "\n",
    "    for i in range(len(colList)):\n",
    "        value = colList[i]        \n",
    "        d[value] = [data[data[col].str.contains(colList[i])]['Actors'].values]\n",
    "\n",
    "    data_export = pd.DataFrame.from_dict(d, orient='index', columns=['Actors']).head(top)\n",
    "    \n",
    "    return data_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def movies_per_day(df, year):    \n",
    "    nMovies = df.loc[year]['Title']\n",
    "\n",
    "    now = pd.Timestamp('now')\n",
    "    if year == now.year:\n",
    "        lastDay = date.today()\n",
    "    else:\n",
    "        lastDay = date(year, 12, 31)\n",
    "    \n",
    "    firstDay = date(year, 1, 1)\n",
    "    delta = lastDay - firstDay\n",
    "\n",
    "    moviesPerDay = nMovies / (delta.days + 1) # 1st of January\n",
    "    print(moviesPerDay.round(2), 'movies per day in', year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def getRatio(a, b):\n",
    "    a = a.lower()\n",
    "    b = b.lower()\n",
    "    total  = len(a) + len(b)\n",
    "    counts = (Counter(a) - Counter(b)) + (Counter(b) - Counter(a))\n",
    "    return 100 - 100 * sum(counts.values()) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/56603572/grouping-similar-strings-together-from-a-list\n",
    "\n",
    "#data = ['MONTREAL EDUCATION BOARD', 'Ile de Montreal', 'Montreal',\n",
    "#       'Ville de Montreal', 'MONTREAL CITY', 'Monrteal', 'Mont-real',\n",
    "#       'Toronto', 'Toronto city', 'Tornoto', 'What is this', 'Bananasplit',\n",
    "#       'Banana', 'StLouis', 'St Louis', 'Saint Louis']\n",
    "\n",
    "def pair_groups(data, threshold, minGroupSize):\n",
    "\n",
    "    paired = { c:{c} for c in data }\n",
    "    for a, b in combinations(data, 2):\n",
    "        if getRatio(a,b) < threshold: continue\n",
    "        paired[a].add(b)\n",
    "        paired[b].add(a)\n",
    "\n",
    "    groups    = list()\n",
    "    ungrouped = set(data)\n",
    "    while ungrouped:\n",
    "        bestGroup = {}\n",
    "        for city in ungrouped:\n",
    "            g = paired[city] & ungrouped\n",
    "            for c in g.copy():\n",
    "                g &= paired[c] \n",
    "            if len(g) > len(bestGroup):\n",
    "                bestGroup = g\n",
    "        if len(bestGroup) < minGroupSize : break  # to terminate grouping early change minGroupSize to 3\n",
    "        ungrouped -= bestGroup\n",
    "        groups.append(bestGroup)\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the weighted rating of each movie\n",
    "def weighted_rating(v, R, m, C):\n",
    "    # v - votes\n",
    "    # R - Rating\n",
    "    # m - minimun number of votes\n",
    "    # C - mean\n",
    "    score = []\n",
    "    # Calculation based on the IMDB formula\n",
    "    score = (v/(v+m) * R) + (m/(m+v) * C).round(3)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def most_watched_genre(df, top, year):\n",
    "    cols = ['Genre','Title','imdbRating','Date']\n",
    "    df_explode_genre = df[df.Date.dt.year == year][cols].copy()\n",
    "    \n",
    "    df_explode_genre.Genre = df_explode_genre.Genre.str.split(',').apply(lambda x: [e.strip() for e in x])\n",
    "    df_explode_genre = explode(df_explode_genre, ['Genre'])\n",
    "    \n",
    "    df_explode_genrerat = get_mean_value(df_explode_genre, 'Genre', 'imdbRating')\n",
    "    \n",
    "    show_all(df_explode_genrerat.nlargest(top, 'Total', keep='all'))\n",
    "    \n",
    "    return df_explode_genrerat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def most_watched_genre_combo(df_, top, year):\n",
    "    cols = ['Genre','Title','imdbRating','Date']\n",
    "    df_genre = df_[df_.Date.dt.year == year][cols].copy()\n",
    "    \n",
    "    gb_genre = df_genre.groupby(by='Genre')\n",
    "    # Genre combination with most movies, showing worst and best rated movies\n",
    "    df_genre_agg = gb_genre.agg({'imdbRating': ['min','max','mean'], \n",
    "                                 'Title': 'count'})\n",
    "    df_genre_agg.reset_index(inplace=True)\n",
    "    \n",
    "    df_genre_agg_worst = gb_genre['imdbRating'].idxmin().fillna(0).astype(int)\n",
    "    df_genre_agg[('Title','worst')] = df_genre.reindex(df_genre_agg_worst).reset_index(drop=True)['Title']\n",
    "    \n",
    "    df_genre_agg_best = gb_genre['imdbRating'].idxmax().fillna(0).astype(int)\n",
    "    df_genre_agg[('Title','best')] = df_genre.reindex(df_genre_agg_best).reset_index(drop=True)['Title']\n",
    "    \n",
    "    df_genre_agg = df_genre_agg.set_index([('Genre','')]).round(1)\n",
    "    df_genre_agg.index.rename('Genre', inplace=True)\n",
    "    \n",
    "    show_all(df_genre_agg.nlargest(top, [('Title','count')], keep='all'))\n",
    "    \n",
    "    return df_genre_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display everything\n",
    "def show_all(df):\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map colors\n",
    "\n",
    "#'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', \n",
    "#'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', \n",
    "#'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', \n",
    "#'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', \n",
    "#'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', \n",
    "#'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', \n",
    "#'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', \n",
    "#'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', \n",
    "#'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r', \n",
    "#'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', \n",
    "#'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', \n",
    "#'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', 'inferno_r', 'jet', 'jet_r', \n",
    "#'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', \n",
    "#'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r', 'seismic', 'seismic_r', \n",
    "#'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', \n",
    "#'terrain', 'terrain_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', \n",
    "#'winter', 'winter_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add value labels\n",
    "def add_labels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y[i], y[i], ha = 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ticks_vbarplot(x, y, ax):\n",
    "    bars = ax.bar(x, y, width=0.8)\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        label_x_pos = bar.get_x() + bar.get_width() / 2\n",
    "        ax.text(label_x_pos, height, s=f'{height}', ha='center', va='bottom')\n",
    "        \n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(12)\n",
    "    \n",
    "    start = min(x)\n",
    "    end = max(x)+1\n",
    "    \n",
    "    #start, end = ax.get_xlim()\n",
    "    ax.xaxis.set_ticks(np.arange(start, end))\n",
    "    ax.tick_params(axis='x', labelsize=10)\n",
    "    #ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1d'))\n",
    "    ax.set_xticklabels(np.arange(start, end), rotation=90)   \n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Draw a heatmap with the numeric values in each cell\n",
    "def plot_ct_heatmap(df, index, year):\n",
    "    plotme = crosstab_by_year(df, index, year)\n",
    "    # Drop row used to organize data\n",
    "    plotme = plotme.droplevel(0, axis=0)\n",
    "    \n",
    "    if index == 'Weekday':\n",
    "        plotme = plotme.iloc[0:7, :-1]\n",
    "        # Drop column used to organize data\n",
    "        plotme = plotme.droplevel(0, axis=1)\n",
    "        ylabel = index\n",
    "\n",
    "        _, ax = plt.subplots(figsize=(9, 5))\n",
    "        plt.title('Movie ' + index + ' count by Month')\n",
    "\n",
    "    elif index == 'Month':\n",
    "        # this uses days instead of weekdays\n",
    "        plotme = plotme.T\n",
    "        ylabel = 'Day'\n",
    "\n",
    "        _, ax = plt.subplots(figsize=(10, 15))\n",
    "        plt.title('Movie day count by ' + index)\n",
    "    \n",
    "    # Plot\n",
    "    sns.heatmap(plotme, annot=True, fmt=\"d\", linewidths=.5, ax=ax)\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create week dataframe with total movies watched by weekday\n",
    "def create_weekday_df(df, year):\n",
    "    # Select 'All' column and remove 'All' row\n",
    "    df_weekday = pd.DataFrame(crosstab_by_year(df, 'Weekday', year)[('All','')]).reset_index().iloc[:-1, :] \n",
    "    # Get columns names ('Weekday' and 'WeekdayName')\n",
    "    df_weekday.columns = df_weekday.columns.droplevel(1)    \n",
    "    # Select and rename columns\n",
    "    df_weekday = df_weekday[['WeekdayName','All']]\n",
    "    df_weekday.rename(columns={\"WeekdayName\": \"Weekday\", \"All\": \"Movies\"}, inplace=True)    \n",
    "    return df_weekday\n",
    "\n",
    "\n",
    "# Movies by Weekday\n",
    "def plot_weekday_bar(df, year):    \n",
    "    df_weekday = create_weekday_df(df, year)\n",
    "\n",
    "    # Prepare plot\n",
    "    norm = plt.Normalize(df_weekday.Movies.min(), df_weekday.Movies.max())\n",
    "    cmap = plt.get_cmap(\"rocket\")\n",
    "    values = df_weekday.Movies.values\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.barplot('Weekday', 'Movies', data=df_weekday, palette=cmap(norm(values)))\n",
    "    plt.title('# Movies watched by Weekday')\n",
    "    plt.ylabel('# Movies')\n",
    "    \n",
    "    for i, n in enumerate(df_weekday['Movies']):\n",
    "        plt.text(i, n+0.3, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies by Week of Year\n",
    "def plot_week_bar(df_, year):  \n",
    "    df_weekofyear = df_[['Title','Date']].copy()\n",
    "    df_weekofyear['Weekofyear'] = df_weekofyear.Date.dt.weekofyear\n",
    "    \n",
    "    # no movies from week 53 in the beginning of the year\n",
    "    temp = df_weekofyear[df_weekofyear.Date.dt.year == year]\n",
    "    temp = temp[~((temp.Date.dt.month == 1) & (temp.Date.dt.weekofyear >= 52))]\n",
    "    \n",
    "    # week starts on monday and some values may fall on week 52 or 53 of previous year\n",
    "    if df_weekofyear[df_weekofyear.Date.dt.year == year+1].size > 0:\n",
    "        appendthis = df_weekofyear[df_weekofyear.Date.dt.year == year+1][df_weekofyear.Date.dt.weekofyear >= 52]\n",
    "        temp = temp.append(appendthis)\n",
    "        \n",
    "    # after filters\n",
    "    df_weekofyear = temp.copy()\n",
    "    gb_df_weekofyear = df_weekofyear.groupby('Weekofyear').agg({'Title':'count'}).reset_index()   \n",
    "\n",
    "    if gb_df_weekofyear.size != 0:\n",
    "        # Prepare plot\n",
    "        norm = plt.Normalize(gb_df_weekofyear.Title.min(), gb_df_weekofyear.Title.max())\n",
    "        cmap = plt.get_cmap(\"magma\")\n",
    "        values = gb_df_weekofyear.Title.values\n",
    "        \n",
    "        # Plotting\n",
    "        ax = plt.figure(figsize=(20,8)).gca()\n",
    "        sns.barplot(x='Weekofyear', y='Title', data=gb_df_weekofyear, palette=cmap(norm(values))) #palette='cool')\n",
    "        ax.yaxis.set_major_locator(ticker.MaxNLocator(integer = True))\n",
    "        \n",
    "        plt.title('# Movies watched by Week of Year (in ' + str(year) +')')\n",
    "        plt.xlabel('Week of Year')\n",
    "        plt.ylabel('# Movies')\n",
    "        \n",
    "        for i, n in enumerate(gb_df_weekofyear['Title']):\n",
    "            plt.text(i, n+0.3, n)\n",
    "            \n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No data!')\n",
    "        \n",
    "    return gb_df_weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how many hours spent watching movies by month\n",
    "def plot_month_bar(df, year):\n",
    "    # Group by month (get .month to maintain month order)\n",
    "    gb_month = [df.Date.dt.month, df.Date.dt.month_name()]\n",
    "    \n",
    "    df_month = df[df.Date.dt.year==year].groupby(gb_month).agg({'Runtime':'sum'}).droplevel(0, axis=0).reset_index()\n",
    "    df_month['Runtime'] = (df_month['Runtime'] / 60).astype(int)\n",
    "    df_month = df_month.rename(columns={'Date':'Month'})\n",
    "\n",
    "    # Prepare plot\n",
    "    norm = plt.Normalize(df_month.Runtime.min(), df_month.Runtime.max())\n",
    "    cmap = plt.get_cmap(\"magma\")\n",
    "    values = df_month.Runtime.values\n",
    "\n",
    "    # Plotting\n",
    "    ax = plt.figure(figsize=(15,8)).gca()\n",
    "    sns.barplot(x='Month',y='Runtime',data=df_month, palette=cmap(norm(values)))\n",
    "    ax.yaxis.set_major_locator(ticker.MaxNLocator(integer = True))\n",
    "    plt.title('# Hours of movies watched by Month (in ' + str(year) +')')\n",
    "    plt.ylabel('# Hours')\n",
    "    \n",
    "    for i, n in enumerate(values):\n",
    "        plt.text(i, n, n)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot swarm box plot with x based on Release Year or Decade\n",
    "def plot_box_swarm(x, y, data, order):\n",
    "    # Plot\n",
    "    _, ax = plt.subplots(figsize=(30, 8))\n",
    "    ax = sns.boxplot(x, y, data=data, order=order)\n",
    "    ax = sns.swarmplot(x, y, data=data, order=order, color=\".4\")\n",
    "    \n",
    "    # add grid lines\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.xaxis.grid(True)\n",
    "   \n",
    "    if x == 'Year':\n",
    "        x = 'Release Year'\n",
    "        ax.set_xlabel(x)\n",
    "        \n",
    "    title = y + ' by ' + x    \n",
    "    ax.set_title(title)\n",
    "    \n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation = 90) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of Movies by Release Year\n",
    "def plot_movie_trend(df, year):\n",
    "    plot_movies_year = df[df.Date.dt.year == year].groupby(by='Year').agg({'Title': 'count'}).reset_index()\n",
    "    x = plot_movies_year.Year\n",
    "    y = plot_movies_year.Title\n",
    "    \n",
    "    # https://towardsdatascience.com/how-to-make-bar-and-hbar-charts-with-labels-using-matplotlib-b701ce70ba9c\n",
    "    _, ax = plt.subplots(figsize=(20, 8))\n",
    "    add_ticks_vbarplot(x, y, ax)     \n",
    "    \n",
    "    plt.title('# Movies watched by Release Year (in ' + str(year) +')')\n",
    "    plt.xlabel('Release Year')\n",
    "    plt.ylabel('# Movies')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
